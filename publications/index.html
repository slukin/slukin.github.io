<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Stephanie M. Lukin</title> <meta name="author" content="Stephanie M. Lukin"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://slukin.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Stephanie </span>M. Lukin</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <p>Publications by research topic in reversed chronological order. <a href="https://scholar.google.com/citations?user=NjiIcLUAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Google scholar page is here.</a> Last updated Nov 2023.</p> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#9392d1"><a href="">Visual Storytelling</a></abbr></div> <div id="halperin2023envisioning" class="col-sm-8"> <div class="title">Envisioning Narrative Intelligence: A Creative Visual Storytelling Anthology</div> <div class="author"> Brett A Halperin, and <em>Stephanie M Lukin</em> </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://dl.acm.org/doi/10.1145/3544548.3580744" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>In this paper, we collect an anthology of 100 visual stories from authors who participated in our systematic creative process of improvised story-building based on image sequences. Following close reading and thematic analysis of our anthology, we present five themes that characterize the variations found in this creative visual storytelling process: (1) Narrating What is in Vision vs. Envisioning; (2) Dynamically Characterizing Entities/Objects; (3) Sensing Experiential Information About the Scenery; (4) Modulating the Mood; (5) Encoding Narrative Biases. In understanding the varied ways that people derive stories from images, we offer considerations for collecting story-driven training data to inform automatic story generation. In correspondence with each theme, we envision narrative intelligence criteria for computational visual storytelling as: creative, reliable, expressive, grounded, and responsible. From these criteria, we discuss how to foreground creative expression, account for biases, and operate in the bounds of visual storyworlds.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">halperin2023envisioning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Envisioning Narrative Intelligence: A Creative Visual Storytelling Anthology}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Halperin, Brett A and Lukin, Stephanie M}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--21}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#9392d1"><a href="">Visual Storytelling</a></abbr></div> <div id="lukin2023see" class="col-sm-8"> <div class="title">SEE&amp;TELL: Controllable Narrative Generation from Images</div> <div class="author"> <em>Stephanie M Lukin</em>, and Sungmin Eum</div> <div class="periodical"> <em>In The AAAI-23 Workshop on Creative AI Across Modalities</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/pdf?id=_8Ity3P03Z1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We propose a visual storytelling framework with a distinction between what is present and observable in the visual storyworld, and what story is ultimately told. We implement a model that tells a story from an image using three affordances: 1) a fixed set of visual properties in an image that constitute a holistic representation its contents, 2) a variable stage direction that establishes the story setting, and 3) incremental questions about character goals. The generated narrative plans are then realized as expressive texts using few-shot learning. Following this approach, we generated 64 visual stories and measured the preservation, loss, and gain of visual information throughout the pipeline, and the willingness of a reader to take action to read more. We report different proportions of visual information preserved and lost depending upon the phase of the pipeline and the stage direction’s apparent relatedness to the image, and report 83% of stories were found to be interesting.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lukin2023see</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SEE&amp;TELL: Controllable Narrative Generation from Images}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lukin, Stephanie M and Eum, Sungmin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The AAAI-23 Workshop on Creative AI Across Modalities}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#9392d1"><a href="">Visual Storytelling</a></abbr></div> <div id="lukin2023learning" class="col-sm-8"> <div class="title">Learning to Understand Anomalous Scenes from Human Interactions</div> <div class="author"> <em>Stephanie M Lukin</em>, Rahul Sharma, and Michael Bellissimo</div> <div class="periodical"> <em>In DEVCOM Army Research Laboratory Technical Report</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://apps.dtic.mil/sti/pdfs/AD1189942.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>At the US Army Combat Capabilities Development Command Army Research Laboratory, we are studying behavior, buildingdata sets, and developing technology for anomaly classification and explanation, in which an autonomous agent generatesnatural language descriptions and interpretations of environments that may contain anomalous properties. This technology willsupport decision-making in uncertain conditions and resilient autonomous maneuvers where a Soldier and robot teammatecomplete exploratory navigation tasks in unknown or dangerous environments under network-constrained circumstances eg, search and rescue following a natural disaster. We detail our contributions in this report as follows we designed an anomalytaxonomy drawing upon related work in visual anomaly detection we designed two experiments taking place in virtualenvironments that were manipulated to exhibit anomalous properties based on the taxonomy we collected a small corpus ofhuman speech and humanrobot dialogue for an anomaly detection and explanation task and finally, we designed a novelannotation schema and applied it to a subset of our corpus.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lukin2023learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to Understand Anomalous Scenes from Human Interactions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lukin, Stephanie M and Sharma, Rahul and Bellissimo, Michael}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{DEVCOM Army Research Laboratory Technical Report}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NLG</abbr></div> <div id="summers2023brainstorm" class="col-sm-8"> <div class="title">Brainstorm, then Select: a Generative Language Model Improves Its Creativity Score</div> <div class="author"> Douglas Summers-Stay, Clare R Voss, and <em>Stephanie M Lukin</em> </div> <div class="periodical"> <em>In The AAAI-23 Workshop on Creative AI Across Modalities</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/pdf?id=8HwKaJ1wvl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Creative problem solving is a crucial ability for intelligent agents. A common method that individuals or groups use to invent creative solutions is to start with a “brainstorming" phase, where many solutions to a problem are proposed, and then to follow with a “selection" phase, where those solutions are judged by some criteria so that the best solutions can be selected. Using the Alternate Uses Task, a test for divergent thinking abilities (a key aspect of creativity) we show that when a large language model is given a sequence of prompts that include both brainstorming and selection phases, its performance improves over brainstorming alone. Furthermore, we show that by following this paradigm, a large language model can even achieve higher than average human performance on the same task. Following our analysis, we propose further research to gain a clearer understanding of what counts as “creativity" in language models.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">summers2023brainstorm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Brainstorm, then Select: a Generative Language Model Improves Its Creativity Score}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Summers-Stay, Douglas and Voss, Clare R and Lukin, Stephanie M}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The AAAI-23 Workshop on Creative AI Across Modalities}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="">HRI</a></abbr></div> <div id="lukin2023navigating" class="col-sm-8"> <div class="title">Navigating to Success in Multi-Modal Human-Robot Collaboration: Corpus and Analysis</div> <div class="author"> <em>Stephanie M. Lukin</em>, Kimbery A. Pollard, Claire Bonial, Taylor Hudson, Ron Artstein, Clare Voss, and David Traum</div> <div class="periodical"> <em>In IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2310.17568" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Human-guided robotic exploration is a useful approach to gathering information at remote locations, especially those that might be too risky, inhospitable, or inaccessible for humans. Maintaining common ground between the remotely-located partners is a challenge, one that can be facilitated by multi-modal communication. In this paper, we explore how participants utilized multiple modalities to investigate a remote location with the help of a robotic partner. Participants issued spoken natural language instructions and received from the robot: text-based feedback, continuous 2D LIDAR mapping, and upon-request static photographs. We noticed that different strategies were adopted in terms of use of the modalities, and hypothesize that these differences may be correlated with success at several exploration sub-tasks. We found that requesting photos may have improved the identification and counting of some key entities (doorways in particular) and that this strategy did not hinder the amount of overall area exploration. Future work with larger samples may reveal the effects of more nuanced photo and dialogue strategies, which can inform the training of robotic agents. Additionally, we announce the release of our unique multi-modal corpus of human-robot communication in an exploration context: SCOUT, the Situated Corpus on Understanding Transactions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lukin2023navigating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Navigating to Success in Multi-Modal Human-Robot Collaboration: Corpus and Analysis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lukin, Stephanie M. and Pollard, Kimbery A. and Bonial, Claire and Hudson, Taylor and Artstein, Ron and Voss, Clare and Traum, David}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="">HRI</a></abbr></div> <div id="marge2023bot" class="col-sm-8"> <div class="title">Bot Language (Summary Technical Report, Oct 2016–Sep 2021)</div> <div class="author"> Matthew Marge, Claire Bonial, <em>Stephanie Lukin</em>, and Clare Voss</div> <div class="periodical"> <em>In DEVCOM Army Research Laboratory Technical Report</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://apps.dtic.mil/sti/trecms/pdf/AD1194759.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This report provides a comprehensive summary of the contributions made as part of the Bot Language project, a 5-year US Army Combat Capabilities Development Command Army Research Laboratory-led initiative in partnership with researchers at the University of Southern Californias Institute for Creative Technologies and Carnegie Mellon University. In particular, this report describes accomplishments funded under the project Naturalistic Behavior for Shared Understanding and Explanation with Intelligent Systems. The goal of this research was to provide more natural ways for people to communicate with robots using language. Our vision was to enable robots to engage in a back-and-forth dialogue with human teammates where robots can provide status updates and ask for clarification where appropriate. To this end, we conducted a phased progression of four experiments where human participants were giving navigation instructions to a remotely located robot, while the robots dialogue and navigation processes were initially controlled by human experimenters. Over the course of the experiments, automation was progressively introduced until dialogue processing was completely driven by a classifier trained on the data collected in previous experiments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">marge2023bot</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Bot Language (Summary Technical Report, Oct 2016--Sep 2021)}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marge, Matthew and Bonial, Claire and Lukin, Stephanie and Voss, Clare}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{DEVCOM Army Research Laboratory Technical Report}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="">HRI</a></abbr></div> <div id="lukin2023anomaly" class="col-sm-8"> <div class="title">Anomaly Detection with Visual Question Answering</div> <div class="author"> <em>Stephanie Lukin</em>, and Rahul Sharma</div> <div class="periodical"> <em>In DEVCOM Army Research Laboratory Technical Report</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://apps.dtic.mil/sti/trecms/pdf/AD1214022.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Anomaly detection is critical for many different use-cases, such as identifying safety hazards to potentially prevent disasters. Developing the capability for a human-robot team to ask targeted questions would be critical to quickly identify a violation of protocol and then quickly take action to rectify the situation. In this report, we experiment with how visual question answering algorithms can be used with a set of carefully constructed questions to detect anomalies in a virtual makerspace and a real-world alleyway. Our exploratory results show improvement over a random baseline and we discuss challenges for future work.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lukin2023anomaly</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Anomaly Detection with Visual Question Answering}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lukin, Stephanie and Sharma, Rahul}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{DEVCOM Army Research Laboratory Technical Report}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="">HRI</a></abbr></div> <div id="higgins2023collaborative" class="col-sm-8"> <div class="title">A Collaborative Building Task in VR vs. Reality</div> <div class="author"> Padraig Higgins, Ryan Barron, <em>Stephanie Lukin</em>, Don Engel, and Cynthia Matuszek</div> <div class="periodical"> <em>In International Symposium on Experimental Robotics (ISER)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Human-robot interaction is a critical area of research, providing support for collaborative tasks where a human instructs a robot to interact with and manipulate objects in an environment. However, an under-explored element of these collaborative manipulation tasks are small-scale building exercises, in which the human and robot are working together in close proximity with the same set of objects. Under these conditions, it is essential to ensure the human’s safety and mitigate comfort risks during the interaction. As there is danger in exposing humans to untested robots, a safe and controlled environment is required. Simulation and virtual reality (VR) for HRI have shown themselves to be suitable tools for creating space for human-robot experimentation that can be beneficial in these scenarios. The primary contributions of this work are as follows. First, we demonstrate a successful small-scale HRI co-manipulation task in the form of allowing a robot and person to jointly build a simple circuit in both simulation and on a physical robot, and examine the success and failure modes of the task in both settings. Second, we compare the user experience and results of task performance in both reality and VR in order to understand how simulated environments can be used for HRI studies of this sort. We conclude that sim-to-real studies that incorporate virtual reality are a feasible mechanism for studying human-robot interactions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">higgins2023collaborative</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Collaborative Building Task in VR vs. Reality}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Higgins, Padraig and Barron, Ryan and Lukin, Stephanie and Engel, Don and Matuszek, Cynthia}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Symposium on Experimental Robotics (ISER)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="">HRI</a></abbr></div> <div id="bonialmaking" class="col-sm-8"> <div class="title">Making Sense of Stop</div> <div class="author"> Claire Bonial, Taylor Hudson, Anthony L Baker, <em>Stephanie M Lukin</em>, and David Traum</div> <div class="periodical"> <em>In Annotation, Recognition and Evaluation of Actions (AREA II) Workshop at ESSLLI</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.areaworkshop.org/wp-content/uploads/2022/07/Bonial-etal-2022.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>The instruction to “stop” in human-robot interactions is packed with multiple interpretations.“Stop” can convey the operator’s intent to indicate where the robot should halt motion, or it can convey the operator’s realization that the robot is not executing an instruction satisfactorily and begin the process of repair. We analyze cases of “stop” in a corpus of humanrobot dialogue, characterizing them along the dimensions of repair status and timing within the interaction, in order to discover patterns and develop design recommendations for how robots should make sense of “stop.”</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bonialmaking</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Making Sense of Stop}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bonial, Claire and Hudson, Taylor and Baker, Anthony L and Lukin, Stephanie M and Traum, David}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Annotation, Recognition and Evaluation of Actions (AREA II) Workshop at ESSLLI}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="bonial2022search" class="col-sm-8"> <div class="title">The Search for Agreement on Logical Fallacy Annotation of an Infodemic</div> <div class="author"> Claire Bonial, Austin Blodgett, Taylor Hudson, <em>Stephanie Lukin</em>, Jeffrey Micher, Douglas Summers-Stay, Peter Sutor, and Clare Voss</div> <div class="periodical"> <em>In Proceedings of the Thirteenth Language Resources and Evaluation Conference</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2022.lrec-1.471.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We evaluate an annotation schema for labeling logical fallacy types, originally developed for a crowd-sourcing annotation paradigm, now using an annotation paradigm of two trained linguist annotators. We apply the schema to a variety of different genres of text relating to the COVID-19 pandemic. Our linguist (as opposed to crowd-sourced) annotation of logical fallacies allows us to evaluate whether the annotation schema category labels are sufficiently clear and non-overlapping for both manual and, later, system assignment. We report inter-annotator agreement results over two annotation phases as well as a preliminary assessment of the corpus for training and testing a machine learning algorithm (Pattern-Exploiting Training) for fallacy detection and recognition. The agreement results and system performance underscore the challenging nature of this annotation task and suggest that the annotation schema and paradigm must be iteratively evaluated and refined in order to arrive at a set of annotation labels that can be reproduced by human annotators and, in turn, provide reliable training data for automatic detection and recognition systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bonial2022search</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Search for Agreement on Logical Fallacy Annotation of an Infodemic}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bonial, Claire and Blodgett, Austin and Hudson, Taylor and Lukin, Stephanie and Micher, Jeffrey and Summers-Stay, Douglas and Sutor, Peter and Voss, Clare}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Thirteenth Language Resources and Evaluation Conference}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4430--4438}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="bonial2022you" class="col-sm-8"> <div class="title">You Can’t Quarantine the Truth: Lessons Learned in Logical Fallacy Annotation of an Infodemic</div> <div class="author"> Claire Bonial, Taylor A Hudson, Austin Blodgett, <em>Stephanie M Lukin</em>, Jeffrey Micher, Douglas Summers-Stay, Peter Sutor, and Clare R Voss</div> <div class="periodical"> <em>In DEVCOM Army Research Laboratory Technical Report</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://apps.dtic.mil/sti/citations/AD1156298" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Given the current COVID-19 infodemic that crosses multiple genres of text, we posit that flagging potentially problematic information PPI retrieved by a semantic search system will be critical to combating mis- or disinformation. This report describes the construction of a COVID-19 corpus and a two-level annotation of logical fallacies in these documents, supplemented with inter-annotator agreement results over two development phases. We also report a preliminary assessment of the corpus for training and testing a machine learning algorithm Pattern-Exploiting Training for fallacy detection and recognition. The agreement results and system performance underscore the challenging nature of this annotation task. We propose targeted improvements for fallacy annotation and conclude that a practical implementation may be to report a documents overall fallacy rate as a measure of its credibility.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bonial2022you</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{You Can't Quarantine the Truth: Lessons Learned in Logical Fallacy Annotation of an Infodemic}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bonial, Claire and Hudson, Taylor A and Blodgett, Austin and Lukin, Stephanie M and Micher, Jeffrey and Summers-Stay, Douglas and Sutor, Peter and Voss, Clare R}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{DEVCOM Army Research Laboratory Technical Report}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#9392d1"><a href="">Visual Storytelling</a></abbr></div> <div id="lukin2021extending" class="col-sm-8"> <div class="title">Extending Generation and Evaluation and Metrics (GEM) to Grounded Natural Language Generation (NLG) Systems and Evaluating their Descriptive Texts Derived from Image Sequences</div> <div class="author"> <em>Stephanie M Lukin</em>, and Clare R Voss</div> <div class="periodical"> <em>In DEVCOM Army Research Laboratory Technical Report</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://apps.dtic.mil/sti/trecms/pdf/AD1149441.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We present here, for consideration in a future Generation and Evaluation and Metrics (GEM) challenge, a graduated, task-based approach to evaluating grounded natural language generation (NLG) systems that generate descriptive texts derived from sequences of input images. We start by characterizing grounded NLG tasks that generate descriptive texts at increasing levels of complexity, then step through examples of these levels with image sequences and facet targets (input) and their derivative descriptive texts (output) from our human-authored data set. For evaluating whether a grounded NLG system is “good enough” for users’ needs, we first ask if the user can recover the images the system used to derive descriptive texts at the relevant, graduated level of complexity. The texts judged as adequate in this image-selection task are then analyzed for their semantic facet units (SFUs), which form the basis for scoring descriptive texts generated by other grounded NLG systems. The image-selection and SFU scoring together constitute the evaluation we are piloting for grounded, data-to-text NLG systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lukin2021extending</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Extending Generation and Evaluation and Metrics (GEM) to Grounded Natural Language Generation (NLG) Systems and Evaluating their Descriptive Texts Derived from Image Sequences}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lukin, Stephanie M and Voss, Clare R}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{DEVCOM Army Research Laboratory Technical Report}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="">HRI</a></abbr></div> <div id="bonial2021context" class="col-sm-8"> <div class="title">Context Is Key: Annotating Situated Dialogue Relations in Multi-floor Dialogue</div> <div class="author"> Claire Bonial, Mitchell Abrams, Anthony L. Baker, Taylor Hudson, <em>Stephanie M. Lukin</em>, David Traum, and Clare R. Voss</div> <div class="periodical"> <em>In Proceedings of the 25th Workshop on the Semantics and Pragmatics of Dialogue</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/pdf?id=InGUkjHlfO" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>In order to account for the features of situated dialogue, we extend a multi-party, multifloor dialogue annotation schema so that it uniquely marks turns with language that must be grounded to the conversational or situational context. We then annotate a dataset of 168 human-robot dialogues using our extended, situated relation schema. Despite the addition of nuanced dialogue relations that reflect the kind of context referenced in the language, our inter-annotator agreement rates remain similar to those of the original annotation schema. Crucially, our updates separate data that can be used to train dialogue systems in essentially any context from those utterances in the data that are only appropriate in a particular situated environment.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bonial2021context</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Context Is Key: Annotating Situated Dialogue Relations in Multi-floor Dialogue}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bonial, Claire and Abrams, Mitchell and Baker, Anthony L. and Hudson, Taylor and Lukin, Stephanie M. and Traum, David and Voss, Clare R.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 25th Workshop on the Semantics and Pragmatics of Dialogue}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#9392d1"><a href="">Visual Storytelling</a></abbr></div> <div id="sapkale2020maintaining" class="col-sm-8"> <div class="title">Maintaining consistency and relevancy in multi-image visual storytelling</div> <div class="author"> Aishwarya Sapkale, and <em>Stephanie M Lukin</em> </div> <div class="periodical"> <em>In DEVCOM Army Research Laboratory Technical Report</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://apps.dtic.mil/sti/pdfs/AD1115439.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This report proposes an approach for visual storytelling across multiple images that prioritize two aspects of narrative generation 1 the retention of narrative consistency between clauses in the generated story and 2 the retention of relevancy between the generated story and the images from which it was derived. We take a structured approach to multi-image visual storytelling that centers around the middle image in a sequence of three. Acting as the focal point, or climax of the narrative, the plot points surrounding this are selected using events from the Atlas of Machine Commonsense ATOMIC corpus for if-then reasoning about daily activities, and then the selected events are subsequently grounded to the images. The result is an architecture that, given an author goal to guide the story in the form of a prompt, will generate a short narrative that retains a narrative arc and does not deviate from the content of the images.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sapkale2020maintaining</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Maintaining consistency and relevancy in multi-image visual storytelling}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sapkale, Aishwarya and Lukin, Stephanie M}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{DEVCOM Army Research Laboratory Technical Report}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="bonial2020infoforager" class="col-sm-8"> <div class="title">InfoForager: Leveraging semantic search with AMR for COVID-19 research</div> <div class="author"> Claire Bonial, <em>Stephanie Lukin</em>, David Doughty, Steven Hill, and Clare Voss</div> <div class="periodical"> <em>In Proceedings of the Second International Workshop on Designing Meaning Representations</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2020.dmr-1.7/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This paper examines how Abstract Meaning Representation (AMR) can be utilized for finding answers to research questions in medical scientific documents, in particular, to advance the study of UV (ultraviolet) inactivation of the novel coronavirus that causes the disease COVID-19. We describe the development of a proof-of-concept prototype tool, InfoForager, which uses AMR to conduct a semantic search, targeting the meaning of the user question, and matching this to sentences in medical documents that may contain information to answer that question. This work was conducted as a sprint over a period of six weeks, and reveals both promising results and challenges in reducing the user search time relating to COVID-19 research, and in general, domain adaption of AMR for this task.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bonial2020infoforager</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{InfoForager: Leveraging semantic search with AMR for COVID-19 research}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bonial, Claire and Lukin, Stephanie and Doughty, David and Hill, Steven and Voss, Clare}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Second International Workshop on Designing Meaning Representations}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{67--77}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="bonial-etal-2020-dialogue" class="col-sm-8"> <div class="title">Dialogue-AMR: Abstract Meaning Representation for Dialogue</div> <div class="author"> Claire Bonial, Lucia Donatelli, Mitchell Abrams, <em>Stephanie M. Lukin</em>, Stephen Tratz, Matthew Marge, Ron Artstein, David Traum, and Clare Voss</div> <div class="periodical"> <em>In Proceedings of the 12th Language Resources and Evaluation Conference</em>, May 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2020.lrec-1.86.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This paper describes a schema that enriches Abstract Meaning Representation (AMR) in order to provide a semantic representation for facilitating Natural Language Understanding (NLU) in dialogue systems. AMR offers a valuable level of abstraction of the propositional content of an utterance; however, it does not capture the illocutionary force or speaker’s intended contribution in the broader dialogue context (e.g., make a request or ask a question), nor does it capture tense or aspect. We explore dialogue in the domain of human-robot interaction, where a conversational robot is engaged in search and navigation tasks with a human partner. To address the limitations of standard AMR, we develop an inventory of speech acts suitable for our domain, and present “Dialogue-AMR”, an enhanced AMR that represents not only the content of an utterance, but the illocutionary force behind it, as well as tense and aspect. To showcase the coverage of the schema, we use both manual and automatic methods to construct the “DialAMR” corpus—a corpus of human-robot dialogue annotated with standard AMR and our enriched Dialogue-AMR schema. Our automated methods can be used to incorporate AMR into a larger NLU pipeline supporting human-robot dialogue.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bonial-etal-2020-dialogue</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dialogue-AMR: {A}bstract {M}eaning {R}epresentation for Dialogue}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bonial, Claire and Donatelli, Lucia and Abrams, Mitchell and Lukin, Stephanie M. and Tratz, Stephen and Marge, Matthew and Artstein, Ron and Traum, David and Voss, Clare}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 12th Language Resources and Evaluation Conference}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Marseille, France}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{European Language Resources Association}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{684--695}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{English}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{979-10-95546-34-4}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#9392d1"><a href="">Visual Storytelling</a></abbr></div> <div id="lukin2019visual" class="col-sm-8"> <div class="title">Visual Understanding and Narration: A Deeper Understanding and Explanation of Visual Scenes</div> <div class="author"> <em>Stephanie M. Lukin</em>, Claire Bonial, and Clare R. Voss</div> <div class="periodical"> <em>Proceedings of the Workshop on Shortcomings in Vision and Language (SiVL)</em>, May 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1906.00038" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We describe the task of Visual Understanding and Narration, in which a robot (or agent) generates text for the images that it collects when navigating its environment, by answering open-ended questions, such as ’what happens, or might have happened, here?’</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">lukin2019visual</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Visual Understanding and Narration: A Deeper Understanding and Explanation of Visual Scenes}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lukin, Stephanie M. and Bonial, Claire and Voss, Clare R.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the Workshop on Shortcomings in Vision and Language (SiVL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="">HRI</a></abbr></div> <div id="bonial2019dialogue" class="col-sm-8"> <div class="title">Dialogue Structure Annotation Guidelines for Army Research Laboratory (ARL) Human-Robot Dialogue Corpus</div> <div class="author"> Claire Bonial, David Traum, Cassidy Henry, <em>Stephanie M. Lukin</em>, Matthew Marge, Ron Artstein, Kimberly A. Pollard, Ashley Foots, Anthony L. Baker, and Clare R. Voss</div> <div class="periodical"> <em>In DEVCOM Army Research Laboratory Technical Report</em>, May 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://apps.dtic.mil/sti/pdfs/AD1082511.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Here we provide detailed guidelines on how to annotate a multifloor human-robot dialogue for structure elements relevant to informing dialogue management in robotic systems. We start with transcribed and time-aligned dialogue data collected from participants and Wizards of Oz across multiple years of an Army Research Laboratory human-robot interaction experiment the Bot Language project. We define structure elements and annotation protocol for marking up these dialogue data, with the aim to inform development of a dialogue management system onboard a robot.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bonial2019dialogue</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dialogue Structure Annotation Guidelines for Army Research Laboratory (ARL) Human-Robot Dialogue Corpus}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bonial, Claire and Traum, David and Henry, Cassidy and Lukin, Stephanie M. and Marge, Matthew and Artstein, Ron and Pollard, Kimberly A. and Foots, Ashley and Baker, Anthony L. and Voss, Clare R.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{AD1082511}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{DEVCOM Army Research Laboratory Technical Report}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#92c9d1"><a href="">Storytelling</a></abbr></div> <div id="lukin2019narrative" class="col-sm-8"> <div class="title">A narrative sentence planner and structurer for domain independent, parameterizable storytelling</div> <div class="author"> <em>Stephanie M Lukin</em>, and Marilyn A Walker</div> <div class="periodical"> <em>Dialogue &amp; Discourse</em>, May 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://core.ac.uk/download/pdf/230776141.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Storytelling is an integral part of daily life and a key part of how we share information and connect with others. The ability to use Natural Language Generation (NLG) to produce stories that are tailored and adapted to the individual reader could have large impact in many different applications. However, one reason that this has not become a reality to date is the NLG story gap, a disconnect between the plan-type representations that story generation engines produce, and the linguistic representations needed by NLG engines. Here we describe Fabula Tales, a storytelling system supporting both story generation and NLG. With manual annotation of texts from existing stories using an intuitive user interface, Fabula Tales automatically extracts the underlying story representation and its accompanying syntactically grounded representation. Narratological and sentence planning parameters are applied to these structures to generate different versions of the story. We show how our storytelling system can alter the story at the sentence level, as well as the discourse level. We also show that our approach can be applied to different kinds of stories by testing our approach on both Aesop’s Fables and first-person blogs posted on social media. The content and genre of such stories varies widely, supporting our claim that our approach is general and domain independent. We then conduct several user studies to evaluate the generated story variations and show that Fabula Tales’ automatically produced variations are perceived as more immediate, interesting, and correct, and are preferred to a baseline generation system that does not use narrative parameters.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">lukin2019narrative</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A narrative sentence planner and structurer for domain independent, parameterizable storytelling}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lukin, Stephanie M and Walker, Marilyn A}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Dialogue \&amp; Discourse}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{34--86}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NLG</abbr></div> <div id="bowden2019data" class="col-sm-8"> <div class="title">Data-driven dialogue systems for social agents</div> <div class="author"> Kevin K Bowden, Shereen Oraby, Amita Misra, Jiaqi Wu, <em>Stephanie Lukin</em>, and Marilyn Walker</div> <div class="periodical"> <em>In Advanced Social Interaction with Agents: 8th International Workshop on Spoken Dialog Systems</em>, May 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="In%20order%20to%20build%20dialogue%20systems%20to%20tackle%20the%20ambitious%20task%20of%20holding%20social%20conversations,%20we%20argue%20that%20we%20need%20a%20data-driven%20approach%20that%20includes%20insight%20into%20human%20conversational%20%E2%80%9Cchit-chat%E2%80%9D,%20and%20which%20incorporates%20different%20natural%20language%20processing%20modules.%20Our%20strategy%20is%20to%20analyze%20and%20index%20large%20corpora%20of%20social%20media%20data,%20including%20Twitter%20conversations,%20online%20debates,%20dialogues%20between%20friends,%20and%20blog%20posts,%20and%20then%20to%20couple%20this%20data%20retrieval%20with%20modules%20that%20perform%20tasks%20such%20as%20sentiment%20and%20style%20analysis,%20topic%20modeling,%20and%20summarization.%20We%20aim%20for%20personal%20assistants%20that%20can%20learn%20more%20nuanced%20human%20language,%20and%20to%20grow%20from%20task-oriented%20agents%20to%20more%20personable%20social%20bots." class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>https://www.academia.edu/download/54168761/IWSDS2017_paper_33.pdf</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bowden2019data</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Data-driven dialogue systems for social agents}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bowden, Kevin K and Oraby, Shereen and Misra, Amita and Wu, Jiaqi and Lukin, Stephanie and Walker, Marilyn}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advanced Social Interaction with Agents: 8th International Workshop on Spoken Dialog Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{53--56}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="bonial2019augmenting" class="col-sm-8"> <div class="title">Augmenting Abstract Meaning Representation for Human-Robot Dialogue</div> <div class="author"> Claire Bonial, Lucia Donatelli, <em>Stephanie M. Lukin</em>, Stephen Tratz, Ron Artstein, David Traum, and Clare R. Voss</div> <div class="periodical"> <em>In Proceedings of the First International Workshop on Designing Meaning Representations (DMR)</em>, Aug 2019 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="">HRI</a></abbr></div> <div id="bonial-area" class="col-sm-8"> <div class="title">Human-Robot Dialogue and Collaboration in Search and Navigation</div> <div class="author"> Claire Bonial, <em>Stephanie Lukin</em>, Ashley Foots, Cassidy Henry, Matthew Marge, Kimberly Pollard, Ron Artstein, David Traum, and Clare R. Voss</div> <div class="periodical"> <em>In Proceedings of the Annotation, Recognition and Evaluation of Actions (AREA) Workshop at LREC, 2018</em>, Aug 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://research.rug.nl/files/65256401/book_of_proceedings.pdf#page=14" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Collaboration with a remotely located robot in tasks such as disaster relief and search and rescue can be facilitated by grounding natural language task instructions into actions executable by the robot in its current physical context. The corpus we describe here provides insight into the translation and interpretation a natural language instruction undergoes starting from verbal human intent, to understanding and processing, and ultimately, to robot execution. We use a ‘Wizard-of-Oz’methodology to elicit the corpus data in which a participant speaks freely to instruct a robot on what to do and where to move through a remote environment to accomplish collaborative search and navigation tasks. This data offers the potential for exploring and evaluating action models by connecting natural language instructions to execution by a physical robot (controlled by a human ‘wizard’). In this paper, a description of the corpus (soon to be openly available) and examples of actions in the dialogue are provided.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bonial-area</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Human-Robot Dialogue and Collaboration in Search and Navigation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bonial, Claire and Lukin, Stephanie and Foots, Ashley and Henry, Cassidy and Marge, Matthew and Pollard, Kimberly and Artstein, Ron and Traum, David and Voss, Clare R.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Annotation, Recognition and Evaluation of Actions (AREA) Workshop at LREC, 2018}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="">HRI</a></abbr></div> <div id="lukin2018scoutbot" class="col-sm-8"> <div class="title">ScoutBot: A Dialogue System for Collaborative Navigation</div> <div class="author"> <em>Stephanie M. Lukin</em>, Felix Gervits, Cory J. Hayes, Anton Leuski, Pooja Moolchandani, John G. Rogers, Carlos Sanchez Amaro, Matthew Marge, Clare R. Voss, and David Traum</div> <div class="periodical"> <em>In Proceedings of ACL 2018, System Demonstrations</em>, Jul 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/P18-4.pdf#page=107" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>ScoutBot is a dialogue interface to physical and simulated robots that supports collaborative exploration of environments. The demonstration will allow users to issue unconstrained spoken language commands to ScoutBot. ScoutBot will prompt for clarification if the user’s instruction needs additional input. It is trained on human-robot dialogue collected from Wizard-of-Oz experiments, where robot responses were initiated by a human wizard in previous interactions. The demonstration will show a simulated ground robot (Clearpath Jackal) in a simulated environment supported by ROS (Robot Operating System).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lukin2018scoutbot</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lukin, Stephanie M. and Gervits, Felix and Hayes, Cory J. and Leuski, Anton and Moolchandani, Pooja and Rogers, III, John G. and Sanchez Amaro, Carlos and Marge, Matthew and Voss, Clare R. and Traum, David}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{ScoutBot: A Dialogue System for Collaborative Navigation}}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of ACL 2018, System Demonstrations}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Melbourne, Australia}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{93--98}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="">HRI</a></abbr></div> <div id="marge2018balancing" class="col-sm-8"> <div class="title">Balancing Efficiency and Coverage in Human-Robot Dialogue Collection</div> <div class="author"> Matthew Marge, Claire Bonial, <em>Stephanie Lukin</em>, Cory Hayes, Ashley Foots, Ron Artstein, Cassidy Henry, Kimberly Pollard, Carla Gordon, Felix Gervits, Anton Leuski, Susan Hill, Clare Voss, and David Traum</div> <div class="periodical"> <em>In Proceedings of AI-HRI AAAI-FSS</em>, Jul 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.researchgate.net/profile/Kimberly-Pollard-2/publication/347986753_Balancing_Efficiency_and_Coverage_in_Human-Robot_Dialogue_Collection/links/5feb9dc3299bf1408859ccc0/Balancing-Efficiency-and-Coverage-in-Human-Robot-Dialogue-Collection.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We describe a multi-phased Wizard-of-Oz approach to collecting human-robot dialogue in a collaborative search and navigation task. The data is being used to train an initial automated robot dialogue system to support collaborative exploration tasks. In the first phase, a wizard freely typed robot utterances to human participants. For the second phase, this data was used to design a GUI that includes buttons for the most common communications, and templates for communications with varying parameters. Comparison of the data gathered in these phases show that the GUI enabled a faster pace of dialogue while still maintaining high coverage of suitable responses, enabling more efficient targeted data collection, and improvements in natural language understanding using GUI-collected data. As a promising first step towards interactive learning, this work shows that our approach enables the collection of useful training data for navigation-based HRI tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">marge2018balancing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Balancing Efficiency and Coverage in Human-Robot Dialogue Collection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marge, Matthew and Bonial, Claire and Lukin, Stephanie and Hayes, Cory and Foots, Ashley and Artstein, Ron and Henry, Cassidy and Pollard, Kimberly and Gordon, Carla and Gervits, Felix and Leuski, Anton and Hill, Susan and Voss, Clare and Traum, David}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of AI-HRI AAAI-FSS}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="">HRI</a></abbr></div> <div id="henrybot" class="col-sm-8"> <div class="title">The Bot Language Project: Moving Towards Natural Dialogue with Robots</div> <div class="author"> Cassidy Henry, <em>Stephanie Lukin</em>, Kimberly A Pollard, Claire Bonial, Ashley Foots, Ron Artstein, Clare R Voss, David Traum, Matthew Marge, Cory J Hayes, and Susan J. Hill</div> <div class="periodical"> <em>In Proc. of SoCalNLP Symbosium</em>, Jul 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://www.cassidyhenry.com/papers/socalnlp18.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This paper describes an ongoing project investigating bidirectional human-robot NL dialogue with the goal of providing more natural ways for humans to interact with robots. We present the experiment’s resulting corpus, current findings, and future work towards a fully automated system.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">henrybot</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Bot Language Project: Moving Towards Natural Dialogue with Robots}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proc. of SoCalNLP Symbosium}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Henry, Cassidy and Lukin, Stephanie and Pollard, Kimberly A and Bonial, Claire and Foots, Ashley and Artstein, Ron and Voss, Clare R and Traum, David and Marge, Matthew and Hayes, Cory J and Hill, Susan J.}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="">HRI</a></abbr></div> <div id="henryfaster" class="col-sm-8"> <div class="title">Faster Pace in Human-Robot Dialogue Leads to Fewer Dialogue Overlaps</div> <div class="author"> Cassidy Henry, Carla Gordon, David Traum, <em>Stephanie M. Lukin</em>, Kimberly A. Pollard, Ron Artstein, Claire Bonial, Clare R. Voss, Ashley Foots, and Matthew Marge</div> <div class="periodical"> <em>In Proc. of NAACL Workshop on Widening NLP</em>, Jul 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://cassidyhenry.com/papers/winlp18_chenry.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>In this paper, dialogue overlap at the transaction unit structure level is examined. In particular we investigate a corpus of multi-floor dialogue in a human-robot navigation domain. Two conditions are contrasted: a human wizard typing with a keyboard vs using a constricted GUI. The GUI condition leads to more utterances and transaction units, but leads to less overlap at the transaction unit level.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">henryfaster</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Faster Pace in Human-Robot Dialogue Leads to Fewer Dialogue Overlaps}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Henry, Cassidy and Gordon, Carla and Traum, David and Lukin, Stephanie M. and Pollard, Kimberly A. and Artstein, Ron and Bonial, Claire and Voss, Clare R. and Foots, Ashley and Marge, Matthew}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proc. of NAACL Workshop on Widening NLP}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="">HRI</a></abbr></div> <div id="pollard2018we" class="col-sm-8"> <div class="title">How we talk with robots: Eliciting minimally-constrained speech to build natural language interfaces and capabilities</div> <div class="author"> Kimberly A Pollard, <em>Stephanie M Lukin</em>, Matthew Marge, Ashley Foots, and Susan G Hill</div> <div class="periodical"> <em>In Proceedings of the Human Factors and Ergonomics Society Annual Meeting</em>, Jul 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.researchgate.net/profile/Kimberly-Pollard-2/publication/335565997_How_We_Talk_with_Robots_Eliciting_Minimally-Constrained_Speech_to_Build_Natural_Language_Interfaces_and_Capabilities/links/5d6d462b299bf1808d610da9/How-We-Talk-with-Robots-Eliciting-Minimally-Constrained-Speech-to-Build-Natural-Language-Interfaces-and-Capabilities.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Industry, military, and academia are showing increasing interest in collaborative human-robot teaming in a variety of task contexts. Designing effective user interfaces for human-robot interaction is an ongoing challenge, and a variety of single and multiple-modality interfaces have been explored. Our work is to develop a bi-directional natural language interface for remote human-robot collaboration in physically situated tasks. When combined with a visual interface and audio cueing, we intend for the natural language interface to provide a naturalistic user experience that requires little training. Building the language portion of this interface requires first understanding how potential users would speak to the robot. In this paper, we describe our elicitation of minimally-constrained robot-directed language, observations about the users’ language behavior, and future directions for constructing an automated robotic system that can accommodate these language needs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pollard2018we</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{How we talk with robots: Eliciting minimally-constrained speech to build natural language interfaces and capabilities}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pollard, Kimberly A and Lukin, Stephanie M and Marge, Matthew and Foots, Ashley and Hill, Susan G}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Human Factors and Ergonomics Society Annual Meeting}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{62}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{160--164}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{SAGE Publications Sage CA: Los Angeles, CA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="">HRI</a></abbr></div> <div id="lukin-etal-2018-consequences" class="col-sm-8"> <div class="title">Consequences and Factors of Stylistic Differences in Human-Robot Dialogue</div> <div class="author"> <em>Stephanie Lukin</em>, Kimberly Pollard, Claire Bonial, Matthew Marge, Cassidy Henry, Ron Artstein, David Traum, and Clare Voss</div> <div class="periodical"> <em>In Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</em>, Jul 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sigdial.org/files/workshops/conference19/proceedings/pdf/SIGdial12.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This paper identifies stylistic differences in instruction-giving observed in a corpus of human-robot dialogue. Differences in verbosity and structure (i.e., single-intent vs. multi-intent instructions) arose naturally without restrictions or prior guidance on how users should speak with the robot. Different styles were found to produce different rates of miscommunication, and correlations were found between style differences and individual user variation, trust, and interaction experience with the robot. Understanding potential consequences and factors that influence style can inform design of dialogue systems that are robust to natural variation from human users.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lukin-etal-2018-consequences</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Consequences and Factors of Stylistic Differences in Human-Robot Dialogue}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lukin, Stephanie and Pollard, Kimberly and Bonial, Claire and Marge, Matthew and Henry, Cassidy and Artstein, Ron and Traum, David and Voss, Clare}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 19th Annual {SIG}dial Meeting on Discourse and Dialogue}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Melbourne, Australia}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{110--118}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="">HRI</a></abbr></div> <div id="traum2018dialogue" class="col-sm-8"> <div class="title">Dialogue Structure Annotation for Multi-Floor Interaction</div> <div class="author"> David Traum, Cassidy Henry, <em>Stephanie Lukin</em>, Ron Artstein, Felix Gervits, Kimberly Pollard, Claire Bonial, Su Lei, Clare Voss, Matthew Marge, Cory Hayes, and Susan Hill</div> <div class="periodical"> <em>In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC)</em>, May 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/L18-1017.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We present an annotation scheme for meso-level dialogue structure, specifically designed for multi-floor dialogue. The scheme includes a transaction unit that clusters utterances from multiple participants and floors into units according to realization of an initiator’s intent, and relations between individual utterances within the unit. We apply this scheme to annotate a corpus of multi-floor human-robot interaction dialogues. We examine the patterns of structure observed in these dialogues and present inter-annotator statistics and relative frequencies of types of relations and transaction units. Finally, some example applications of these annotations are introduced.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">traum2018dialogue</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Traum, David and Henry, Cassidy and Lukin, Stephanie and Artstein, Ron and Gervits, Felix and Pollard, Kimberly and Bonial, Claire and Lei, Su and Voss, Clare and Marge, Matthew and Hayes, Cory and Hill, Susan}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dialogue Structure Annotation for Multi-Floor Interaction}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Miyazaki, Japan}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{chair), Nicoletta Calzolari (Conference and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Hasida, Koiti and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios and Tokunaga, Takenobu}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{European Language Resources Association (ELRA)}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{979-10-95546-00-9}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{english}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{104--111}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NLG</abbr></div> <div id="oraby2018controlling" class="col-sm-8"> <div class="title">Controlling Personality-Based Stylistic Variation with Neural Natural Language Generators</div> <div class="author"> Shereen Oraby, Lena Reed, Shubhangi Tandon, TS Sharath, <em>Stephanie Lukin</em>, and Marilyn Walker</div> <div class="periodical"> <em>SIGdial Meeting on Discourse and Dialogue (SIGDIAL)</em>, May 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1805.08352" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Natural language generators for task-oriented dialogue must effectively realize system dialogue actions and their associated semantics. In many applications, it is also desirable for generators to control the style of an utterance. To date, work on task-oriented neural generation has primarily focused on semantic fidelity rather than achieving stylistic goals, while work on style has been done in contexts where it is difficult to measure content preservation. Here we present three different sequence-to-sequence models and carefully test how well they disentangle content and style. We use a statistical generator, Personage, to synthesize a new corpus of over 88,000 restaurant domain utterances whose style varies according to models of personality, giving us total control over both the semantic content and the stylistic variation in the training data. We then vary the amount of explicit stylistic supervision given to the three models. We show that our most explicit model can simultaneously achieve high fidelity to both semantic and stylistic goals: this model adds a context vector of 36 stylistic parameters as input to the hidden state of the encoder at each time step, showing the benefits of explicit stylistic supervision, even when the amount of training data is large.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">oraby2018controlling</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Controlling Personality-Based Stylistic Variation with Neural Natural Language Generators}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Oraby, Shereen and Reed, Lena and Tandon, Shubhangi and Sharath, TS and Lukin, Stephanie and Walker, Marilyn}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{SIGdial Meeting on Discourse and Dialogue (SIGDIAL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NLG</abbr></div> <div id="tandon2018tnt" class="col-sm-8"> <div class="title">TNT-NLG, System 2: Data repetition and meaning representation manipulation to improve neural generation</div> <div class="author"> Shubhangi Tandon, TS Sharath, Shereen Oraby, Lena Reed, <em>Stephanie Lukin</em>, and Marilyn Walker</div> <div class="periodical"> <em>E2E NLG Challenge System Descriptions</em>, May 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ufal.mff.cuni.cz/%C2%A0odusek/E2E/final_papers/E2E-TNT_NLG2.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>End-to-End (E2E) neural models that learn and generate natural language sentence realizations in one step have recently received a great deal of interest from the natural language generation (NLG) community. In this paper, we present “TNT-NLG” System 2, our second system submission in the E2E NLG challenge, which focuses on generating coherent natural language realizations from meaning representations (MRs) in the restaurant domain. We tackle the problem of improving the output of a neural generator based on the open-source baseline model from Dusek et al. (2016) by vastly expanding the training data size by repetition of instances in training, and permutation of the MR. We see that simple modifications allow for increases in performance by providing the generator with a much larger sample of data for learning. Our system is evaluated using quantitative metrics and qualitative human evaluation, and scores competitively in the challenge.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tandon2018tnt</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{TNT-NLG, System 2: Data repetition and meaning representation manipulation to improve neural generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tandon, Shubhangi and Sharath, TS and Oraby, Shereen and Reed, Lena and Lukin, Stephanie and Walker, Marilyn}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{E2E NLG Challenge System Descriptions}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NLG</abbr></div> <div id="oraby2018tnt" class="col-sm-8"> <div class="title">TNT-NLG, System 1: Using a statistical NLG to massively augment crowd-sourced data for neural generation</div> <div class="author"> Shereen Oraby, Lena Reed, Shubhangi Tandon, Sharath TS, <em>Stephanie Lukin</em>, and Marilyn Walker</div> <div class="periodical"> <em>E2E NLG Challenge System Descriptions</em>, May 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ufal.mff.cuni.cz/%C2%A0odusek/E2E/final_papers/E2E-TNT_NLG1.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Ever since the successful application of sequence to sequence learning for neural machine translation systems (Sutskever et al., 2014), interest has surged in its applicability towards language generation in other problem domains. In the area of natural language generation (NLG), there has been a great deal of interest in end-to-end (E2E) neural models that learn and generate natural language sentence realizations in one step. In this paper, we present TNT-NLG System 1, our first system submission to the E2E NLG Challenge, where we generate natural language (NL) realizations from meaning representations (MRs) in the restaurant domain by massively expanding the training dataset. We develop two models for this system, based on Dusek et al.’s (2016a) open source baseline model and context-aware neural language generator. Starting with the MR and NL pairs from the E2E generation challenge dataset, we explode the size of the training set using PERSONAGE (Mairesse and Walker, 2010), a statistical generator able to produce varied realizations from MRs, and use our expanded data as contextual input into our models. We present evaluation results using automated and human evaluation metrics, and describe directions for future work.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">oraby2018tnt</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{TNT-NLG, System 1: Using a statistical NLG to massively augment crowd-sourced data for neural generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Oraby, Shereen and Reed, Lena and Tandon, Shubhangi and TS, Sharath and Lukin, Stephanie and Walker, Marilyn}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{E2E NLG Challenge System Descriptions}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#9392d1"><a href="">Visual Storytelling</a></abbr></div> <div id="lukin2018pipeline" class="col-sm-8"> <div class="title">A Pipeline for Creative Visual Storytelling</div> <div class="author"> <em>Stephanie M. Lukin</em>, Reginald Hobbs, and Clare R. Voss</div> <div class="periodical"> <em>Proceedings of the First Workshop on Storytelling (StoryNLP)</em>, May 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/W18-15.pdf#page=30" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Computational visual storytelling produces a textual description of events and interpretations depicted in a sequence of images. These texts are made possible by advances and cross-disciplinary approaches in natural language processing, generation, and computer vision. We define a computational creative visual storytelling as one with the ability to alter the telling of a story along three aspects: to speak about different environments, to produce variations based on narrative goals, and to adapt the narrative to the audience. These aspects of creative storytelling and their effect on the narrative have yet to be explored in visual storytelling. This paper presents a pipeline of task-modules, Object Identification, Single-Image Inferencing, and Multi-Image Narration, that serve as a preliminary design for building a creative visual storyteller. We have piloted this design for a sequence of images in an annotation task. We present and analyze the collected corpus and describe plans towards automation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">lukin2018pipeline</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Pipeline for Creative Visual Storytelling}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lukin, Stephanie M. and Hobbs, Reginald and Voss, Clare R.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the First Workshop on Storytelling (StoryNLP)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#92c9d1"><a href="">Storytelling</a></abbr></div> <div id="lukin2017doctor" class="col-sm-8"> <div class="title">Generating variations in a virtual storyteller</div> <div class="author"> <em>Stephanie M Lukin</em> </div> <div class="periodical"> <em>University of California, Santa Cruz</em>, May 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://escholarship.org/content/qt0mf0f4js/qt0mf0f4js_noSplash_944974fccb1a37d0dae20428024b9632.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This dissertation introduces the Expressive-Story Translator (EST) content planner and Fabula Tales sentence planner in a storytelling natural language generation framework. Both planners operate in a domain independent manner, abstractly modeling a variety of stories regardless of story vocabulary. The EST captures story semantics from a narrative representation and constructs text plans to maintain semantic content through rhetorical relations. Content planning is performed using these relations to enhance narrative effects, such as modeling emotions and temporal reordering. The EST transforms the story into semantic-syntactic structures interpreted by the parameterizable sentence planner, Fabula Tales. The semantic-syntactic integration allows Fabula Tales to employ narrative sentence planning devices to change narrator point of view, insert direct speech acts, and supplement character voice using operations for lexical selection, aggregation, and pragmatic marker insertion. The frameworks are evaluated using traditional machine translation metrics, narrative metrics, and overgenerate and rank to holistically test the effectiveness of each generated retelling. This work shows how different framings affect reader perception of stories and its characters, and uses statistical analysis of reader feedback to build story models tailored for specific narration preferences.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">lukin2017doctor</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Generating variations in a virtual storyteller}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Doctor of Philosophy in COMPUTER SCIENCE}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lukin, Stephanie M}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{University of California, Santa Cruz}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Persuasion</abbr></div> <div id="lukin2017argument" class="col-sm-8"> <div class="title">Argument strength is in the eye of the beholder: Audience effects in persuasion</div> <div class="author"> <em>Stephanie M Lukin</em>, Pranav Anand, Marilyn Walker, and Steve Whittaker</div> <div class="periodical"> <em>European Chapter of the Association for Computational Linguistics (EACL)</em>, May 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://scholar.archive.org/work/erjsgaihrfa3ve6mslqzbwavg4/access/wayback/http://aclweb.org/anthology/E17-1070" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Americans spend about a third of their time online, with many participating in online conversations on social and political issues. We hypothesize that social media arguments on such issues may be more engaging and persuasive than traditional media summaries, and that particular types of people may be more or less convinced by particular styles of argument, e.g. emotional arguments may resonate with some personalities while factual arguments resonate with others. We report a set of experiments testing at large scale how audience variables interact with argument style to affect the persuasiveness of an argument, an under-researched topic within natural language processing. We show that belief change is affected by personality factors, with conscientious, open and agreeable people being more convinced by emotional arguments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">lukin2017argument</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Argument strength is in the eye of the beholder: Audience effects in persuasion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lukin, Stephanie M and Anand, Pranav and Walker, Marilyn and Whittaker, Steve}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{European Chapter of the Association for Computational Linguistics (EACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="">HRI</a></abbr></div> <div id="bonial2017laying" class="col-sm-8"> <div class="title">Laying Down the Yellow Brick Road: Development of a Wizard-of-Oz Interface for Collecting Human-Robot Dialogue</div> <div class="author"> Claire Bonial, Matthew Marge, Ashley Foots, Felix Gervits, Cory J Hayes, Cassidy Henry, Susan G Hill, Anton Leuski, <em>Stephanie M Lukin</em>, Pooja Moolchandani, Kimberly A Pollard, David Traum, and Clare R Voss</div> <div class="periodical"> <em>Symposium on Natural Communication for Human-Robot Collaboration, AAAI FSS</em>, May 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1710.06406" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We describe the adaptation and refinement of a graphical user interface designed to facilitate a Wizard-of-Oz (WoZ) approach to collecting human-robot dialogue data. The data collected will be used to develop a dialogue system for robot navigation. Building on an interface previously used in the development of dialogue systems for virtual agents and video playback, we add templates with open parameters which allow the wizard to quickly produce a wide variety of utterances. Our research demonstrates that this approach to data collection is viable as an intermediate step in developing a dialogue system for physical robots in remote locations from their users - a domain in which the human and robot need to regularly verify and update a shared understanding of the physical environment. We show that our WoZ interface and the fixed set of utterances and templates therein provide for a natural pace of dialogue with good coverage of the navigation domain.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bonial2017laying</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Laying Down the Yellow Brick Road: Development of a Wizard-of-Oz Interface for Collecting Human-Robot Dialogue}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bonial, Claire and Marge, Matthew and Foots, Ashley and Gervits, Felix and Hayes, Cory J and Henry, Cassidy and Hill, Susan G and Leuski, Anton and Lukin, Stephanie M and Moolchandani, Pooja and Pollard, Kimberly A and Traum, David and Voss, Clare R}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Symposium on Natural Communication for Human-Robot Collaboration, AAAI FSS}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#92c9d1"><a href="">Storytelling</a></abbr></div> <div id="lukin2016personabank" class="col-sm-8"> <div class="title">Personabank: A corpus of personal narratives and their story intention graphs</div> <div class="author"> <em>Stephanie M Lukin</em>, Kevin Bowden, Casey Barackman, and Marilyn A Walker</div> <div class="periodical"> <em>International Conference on Language Resources and Evaluation (LREC)</em>, May 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://www.lrec-conf.org/proceedings/lrec2016/pdf/356_Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We present a new corpus, PersonaBank, consisting of 108 personal stories from weblogs that have been annotated with their Story Intention Graphs, a deep representation of the fabula of a story. We describe the topics of the stories and the basis of the Story Intention Graph representation, as well as the process of annotating the stories to produce the Story Intention Graphs and the challenges of adapting the tool to this new personal narrative domain We also discuss how the corpus can be used in applications that retell the story using different styles of tellings, co-tellings, or as a content planner.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">lukin2016personabank</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Personabank: A corpus of personal narratives and their story intention graphs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lukin, Stephanie M and Bowden, Kevin and Barackman, Casey and Walker, Marilyn A}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Language Resources and Evaluation (LREC)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#92c9d1"><a href="">Storytelling</a></abbr></div> <div id="lukin2015generating" class="col-sm-8"> <div class="title">Generating sentence planning variations for story telling</div> <div class="author"> <em>Stephanie M Lukin</em>, Lena I Reed, and Marilyn A Walker</div> <div class="periodical"> <em>SIGdial Meeting on Discourse and Dialogue (SIGDIAL)</em>, May 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/W15-4627.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>There has been a recent explosion in applications for dialogue interaction ranging from direction-giving and tourist information to interactive story systems. Yet the natural language generation (NLG) component for many of these systems remains largely handcrafted. This limitation greatly restricts the range of applications; it also means that it is impossible to take advantage of recent work in expressive and statistical language generation that can dynamically and automatically produce a large number of variations of given content. We propose that a solution to this problem lies in new methods for developing language generation resources. We describe the ES-TRANSLATOR, a computational language generator that has previously been applied only to fables, and quantitatively evaluate the domain independence of the EST by applying it to personal narratives from weblogs. We then take advantage of recent work on language generation to create a parameterized sentence planner for story generation that provides aggregation operations, variations in discourse and in point of view. Finally, we present a user evaluation of different personal narrative retellings</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#92c9d1"><a href="">Storytelling</a></abbr></div> <div id="lukin2015narrative" class="col-sm-8"> <div class="title">Narrative variations in a virtual storyteller</div> <div class="author"> <em>Stephanie M Lukin</em>, and Marilyn A Walker</div> <div class="periodical"> <em>In Intelligent Virtual Agents: 15th International Conference, (IVA)</em>, May 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.researchgate.net/profile/Marilyn-Walker-5/publication/280297613_Narrative_Variations_in_a_Virtual_Storyteller/links/55aff92a08aeb0ab46698352/Narrative-Variations-in-a-Virtual-Storyteller.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Research on storytelling over the last 100 years has distinguished at least two levels of narrative representation (1) story, or fabula; and (2) discourse, or sujhet. We use this distinction to create Fabula Tales, a computational framework for a virtual storyteller that can tell the same story in different ways through the implementation of general narratological variations, such as varying direct vs. indirect speech, character voice (style), point of view, and focalization. A strength of our computational framework is that it is based on very general methods for re-using existing story content, either from fables or from personal narratives collected from blogs. We first explain how a simple annotation tool allows naíve annotators to easily create a deep representation of fabula called a story intention graph, and show how we use this representation to generate story tellings automatically. Then we present results of two studies testing our narratological parameters, and showing that different tellings affect the reader’s perception of the story and characters.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lukin2015narrative</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Narrative variations in a virtual storyteller}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lukin, Stephanie M and Walker, Marilyn A}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Intelligent Virtual Agents: 15th International Conference, (IVA)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{320--331}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#a777c9"><a href="">Sarcasm</a></abbr></div> <div id="lukin2014identifying" class="col-sm-8"> <div class="title">Identifying Subjective and Figurative Language in Online Dialogue</div> <div class="author"> <em>Stephanie M Lukin</em>, Luke Eisenberg, Thomas Corcoran, and Marilyn A Walker</div> <div class="periodical"> <em>Pacific Northwest Regional NLP Workshop (NWNLP)</em>, May 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.researchgate.net/profile/Stephanie-Lukin/publication/319349901_Identifying_Subjective_and_Figurative_Language_in_Online_Dialogue/links/59f001cfa6fdcce2096dcecf/Identifying-Subjective-and-Figurative-Language-in-Online-Dialogue.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>More and more of the information on the web is dialogic, from Facebook newsfeeds, to forum conversations, to comment threads on news articles. In contrast to traditional, monologic resources such as news, highly social dialogue is very frequent in social media. We aim to automatically identify sarcastic and nasty utterances in unannotated online dialogue, extending a bootstrapping method previously applied to the classification of monologic subjective sentences in Riloff and Weibe 2003. We have adapted the method to fit the sarcastic and nasty dialogic domain. Our method is as follows: 1) Explore methods for identifying sarcastic and nasty cue words and phrases in dialogues; 2) Use the learned cues to train a sarcastic (nasty) Cue-Based Classifier; 3) Learn general syntactic extraction patterns from the sarcastic (nasty) utterances and define fine-tuned sarcastic patterns to create a Pattern-Based Classifier; 4) Combine both Cue-Based and fine-tuned Pattern-Based Classifiers to maximize precision at the expense of recall and test on unannotated utterances.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">lukin2014identifying</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Identifying Subjective and Figurative Language in Online Dialogue}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lukin, Stephanie M and Eisenberg, Luke and Corcoran, Thomas and Walker, Marilyn A}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Pacific Northwest Regional NLP Workshop (NWNLP)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#a777c9"><a href="">Sarcasm</a></abbr></div> <div id="swanson2014getting" class="col-sm-8"> <div class="title">Getting Reliable Annotations for Sarcasm in Online Dialogues</div> <div class="author"> Reid Swanson, <em>Stephanie Lukin</em>, Luke Eisenberg, Thomas Corcoran, and Marilyn Walker</div> <div class="periodical"> <em>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14)</em>, May 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/1063_Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>The language used in online forums differs in many ways from that of traditional language resources such as news. One difference is the use and frequency of nonliteral, subjective dialogue acts such as sarcasm. Whether the aim is to develop a theory of sarcasm in dialogue, or engineer automatic methods for reliably detecting sarcasm, a major challenge is simply the difficulty of getting enough reliably labelled examples. In this paper we describe our work on methods for achieving highly reliable sarcasm annotations from untrained annotators on Mechanical Turk. We explore the use of a number of common statistical reliability measures, such as Kappa, Karger’s, Majority Class, and EM. We show that more sophisticated measures do not appear to yield better results for our data than simple measures such as assuming that the correct label is the one that a majority of Turkers apply</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">swanson2014getting</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Getting Reliable Annotations for Sarcasm in Online Dialogues}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Swanson, Reid and Lukin, Stephanie and Eisenberg, Luke and Corcoran, Thomas and Walker, Marilyn}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4250--4257}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#a777c9"><a href="">Sarcasm</a></abbr></div> <div id="justo2014extracting" class="col-sm-8"> <div class="title">Extracting relevant knowledge for the detection of sarcasm and nastiness in the social web</div> <div class="author"> Raquel Justo, Thomas Corcoran, <em>Stephanie M Lukin</em>, Marilyn Walker, and M Inés Torres</div> <div class="periodical"> <em>Knowledge-Based Systems</em>, May 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0950705114002226?casa_token=p5z2BLY8_RQAAAAA:GeDhy11UmyjAsdE7TsqeKnvkmMNigapm0P1lLSPfL3TIV5IbMN0bppWb-TXuY3d5Gouy%E2%80%93IS0Q" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Automatic detection of emotions like sarcasm or nastiness in online written conversation is a difficult task. It requires a system that can manage some kind of knowledge to interpret that emotional language is being used. In this work, we try to provide this knowledge to the system by considering alternative sets of features obtained according to different criteria. We test a range of different feature sets using two different classifiers. Our results show that the sarcasm detection task benefits from the inclusion of linguistic and semantic information sources, while nasty language is more easily detected using only a set of surface patterns or indicators.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">justo2014extracting</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Extracting relevant knowledge for the detection of sarcasm and nastiness in the social web}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Justo, Raquel and Corcoran, Thomas and Lukin, Stephanie M and Walker, Marilyn and Torres, M In{\'e}s}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Knowledge-Based Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{69}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{124--133}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#92c9d1"><a href="">Storytelling</a></abbr></div> <div id="lukin2014automating" class="col-sm-8"> <div class="title">Automating direct speech variations in stories and games</div> <div class="author"> <em>Stephanie Lukin</em>, James Ryan, and Marilyn Walker</div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</em>, May 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.researchgate.net/profile/Stephanie-Lukin/publication/319391390_Automating_Direct_Speech_Variations_in_Stories_and_Games/links/59f001dc458515c3cc4374fc/Automating-Direct-Speech-Variations-in-Stories-and-Games.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Dialogue authoring in large games requires not only content creation but the subtlety of its delivery, which can vary from character to character. Manually authoring this dialogue can be tedious, time-consuming, or even altogether infeasible. This paper utilizes a rich narrative representation for modeling dialogue and an expressive natural language generation engine for realizing it, and expands upon a translation tool that bridges the two. We add functionality to the translator to allow direct speech to be modeled by the narrative representation, whereas the original translator supports only narratives told by a third person narrator. We show that we can perform character substitution in dialogues. We implement and evaluate a potential application to dialogue implementation: generating dialogue for games with big, dynamic, or procedurally-generated open worlds. We present a pilot study on human perceptions of the personalities of characters using direct speech, assuming unknown personality types at the time of authoring.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lukin2014automating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automating direct speech variations in stories and games}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lukin, Stephanie and Ryan, James and Walker, Marilyn}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{18--23}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="">HRI</a></abbr></div> <div id="lukin2014building" class="col-sm-8"> <div class="title">Building community and commitment with a virtual coach in mobile wellness programs</div> <div class="author"> <em>Stephanie M Lukin</em>, G Michael Youngblood, Honglu Du, and Marilyn Walker</div> <div class="periodical"> <em>In Intelligent Virtual Agents: 14th International Conference, IVA 2014, Boston, MA, USA, August 27-29, 2014. Proceedings 14</em>, May 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.researchgate.net/profile/Marilyn-Walker-5/publication/300361461_Building_Community_and_Commitment_with_a_Virtual_Coach_in_Mobile_Wellness_Programs/links/599fca9b0f7e9b36390366c4/Building-Community-and-Commitment-with-a-Virtual-Coach-in-Mobile-Wellness-Programs.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>FittleBot is virtual coach provided as part of a mobile application named Fittle that aims to provide users with social support and motivation for achieving the user’s health and wellness goals. Fittle’s wellness challenges are based around teams, where each team has its own FittleBot to provide personalized recommendations, support team building and provide information or tips. Here we present a quantitative analysis from a 2-week field study where we test new FittleBot strategies to increase FittleBot’s effectiveness in building team community. Participants using the enhanced FittleBot improved compliance over the two weeks by 8.8% and increased their sense of community by 4%.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lukin2014building</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Building community and commitment with a virtual coach in mobile wellness programs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lukin, Stephanie M and Youngblood, G Michael and Du, Honglu and Walker, Marilyn}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Intelligent Virtual Agents: 14th International Conference, IVA 2014, Boston, MA, USA, August 27-29, 2014. Proceedings 14}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{279--284}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="arouche2014machine" class="col-sm-8"> <div class="title">A machine learning framework for TCP round-trip time estimation</div> <div class="author"> Bruno Astuto Arouche Nunes, Kerry Veenstra, William Ballenthin, <em>Stephanie Lukin</em>, and Katia Obraczka</div> <div class="periodical"> <em>EURASIP Journal on Wireless Communications and Networking</em>, May 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1186/1687-1499-2014-47" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>In this paper, we explore a novel approach to end-to-end round-trip time (RTT) estimation using a machine-learning technique known as the experts framework. In our proposal, each of several ‘experts’ guesses a fixed value. The weighted average of these guesses estimates the RTT, with the weights updated after every RTT measurement based on the difference between the estimated and actual RTT. Through extensive simulations, we show that the proposed machine-learning algorithm adapts very quickly to changes in the RTT. Our results show a considerable reduction in the number of retransmitted packets and an increase in goodput, especially in more heavily congested scenarios. We corroborate our results through ‘live’ experiments using an implementation of the proposed algorithm in the Linux kernel. These experiments confirm the higher RTT estimation accuracy of the machine learning approach which yields over 40% improvement when compared against both standard transmission control protocol (TCP) as well as the well known Eifel RTT estimator. To the best of our knowledge, our work is the first attempt to use on-line learning algorithms to predict network performance and, given the promising results reported here, creates the opportunity of applying on-line learning to estimate other important network variables.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">arouche2014machine</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A machine learning framework for TCP round-trip time estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Arouche Nunes, Bruno Astuto and Veenstra, Kerry and Ballenthin, William and Lukin, Stephanie and Obraczka, Katia}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{EURASIP Journal on Wireless Communications and Networking}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--22}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2013</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#92c9d1"><a href="">Storytelling</a></abbr></div> <div id="rishes2013generating" class="col-sm-8"> <div class="title">Generating different story tellings from semantic representations of narrative</div> <div class="author"> Elena Rishes, <em>Stephanie M Lukin</em>, David K Elson, and Marilyn A Walker</div> <div class="periodical"> <em>In Interactive Storytelling: 6th International Conference, ICIDS 2013, Istanbul, Turkey, November 6-9, 2013, Proceedings 6</em>, May 2013 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1708.08573.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>In order to tell stories in different voices for different audiences, interactive story systems require: (1) a semantic representation of story structure, and (2) the ability to automatically generate story and dialogue from this semantic representation using some form of Natural Language Generation (NLG). However, there has been limited research on methods for linking story structures to narrative descriptions of scenes and story events. In this paper we present an automatic method for converting from Scheherazade’s story intention graph, a semantic representation, to the input required by the personage NLG engine. Using 36 Aesop Fables distributed in DramaBank, a collection of story encodings, we train translation rules on one story and then test these rules by generating text for the remaining 35. The results are measured in terms of the string similarity metrics Levenshtein Distance and BLEU score. The results show that we can generate the 35 stories with correct content: the test set stories on average are close to the output of the Scheherazade realizer, which was customized to this semantic representation. We provide some examples of story variations generated by personage. In future work, we will experiment with measuring the quality of the same stories generated in different voices, and with techniques for making storytelling interactive.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rishes2013generating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Generating different story tellings from semantic representations of narrative}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rishes, Elena and Lukin, Stephanie M and Elson, David K and Walker, Marilyn A}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Interactive Storytelling: 6th International Conference, ICIDS 2013, Istanbul, Turkey, November 6-9, 2013, Proceedings 6}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{192--204}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2013}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#a777c9"><a href="">Sarcasm</a></abbr></div> <div id="lukin2013really" class="col-sm-8"> <div class="title">Really? well. apparently bootstrapping improves the performance of sarcasm and nastiness classifiers for online dialogue</div> <div class="author"> <em>Stephanie Lukin</em>, and Marilyn Walker</div> <div class="periodical"> <em>Workshop on Language Analysis in Social Media (LASM) at NAACL</em>, May 2013 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.academia.edu/download/38733778/W13-1104.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>More and more of the information on the web is dialogic, from Facebook newsfeeds, to forum conversations, to comment threads on news articles. In contrast to traditional, monologic Natural Language Processing resources such as news, highly social dialogue is frequent in social media, making it a challenging context for NLP. This paper tests a bootstrapping method, originally proposed in a monologic domain, to train classifiers to identify two different types of subjective language in dialogue: sarcasm and nastiness. We explore two methods of developing linguistic indicators to be used in a first level classifier aimed at maximizing precision at the expense of recall. The best performing classifier for the first phase achieves 54% precision and 38% recall for sarcastic utterances. We then use general syntactic patterns from previous work to create more general sarcasm indicators, improving precision to 62% and recall to 52%. To further test the generality of the method, we then apply it to bootstrapping a classifier for nastiness dialogic acts. Our first phase, using crowdsourced nasty indicators, achieves 58% precision and 49% recall, which increases to 75% precision and 62% recall when we bootstrap over the first level with generalized syntactic patterns.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">lukin2013really</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Really? well. apparently bootstrapping improves the performance of sarcasm and nastiness classifiers for online dialogue}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lukin, Stephanie and Walker, Marilyn}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Workshop on Language Analysis in Social Media (LASM) at NAACL}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2013}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2011</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="nunes2011machine" class="col-sm-8"> <div class="title">A machine learning approach to end-to-end rtt estimation and its application to tcp</div> <div class="author"> Bruno AA Nunes, Kerry Veenstra, William Ballenthin, <em>Stephanie Lukin</em>, and Katia Obraczka</div> <div class="periodical"> <em>In 2011 Proceedings of 20th International Conference on Computer Communications and Networks (ICCCN)</em>, May 2011 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6006098&amp;casa_token=sS11gmG9mJ4AAAAA:SZFYq4b_O8N7BqPDrBNGBgeFmmiA2PATcUpxeCKpdtiSVVK1sV5hbh3ebwG6pbjZ3LkZUhaQ2A&amp;tag=1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>In this paper, we explore a novel approach to end-toend round-trip time (RTT) estimation using a machine-learning technique known as the Experts Framework. In our proposal, each of several ’experts’ guesses a fixed value. The weighted average of these guesses estimates the RTT, with the weights updated after every RTT measurement based on the difference between the estimated and actual RTT. Through extensive simulations we show that the proposed machine-learning algorithm adapts very quickly to changes in the RTT. Our results show a considerable reduction in the number of retransmitted packets and a increase in goodput, in particular on more heavily congested scenarios. We corroborate our results through “live” experiments using an implementation of the proposed algorithm in the Linux kernel. These experiments confirm the higher accuracy of the machine learning approach with more than 40% improvement, not only over the standard TCP, but also over the well known Eifel RTT estimator</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">nunes2011machine</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A machine learning approach to end-to-end rtt estimation and its application to tcp}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nunes, Bruno AA and Veenstra, Kerry and Ballenthin, William and Lukin, Stephanie and Obraczka, Katia}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2011 Proceedings of 20th International Conference on Computer Communications and Networks (ICCCN)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--6}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2011}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Stephanie M. Lukin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>